import tensorflow as tf
import numpy as np
import gym


class A2C:
    def __init__(self):
        self.game = gym.make('CartPole-v1')
        self.num_actions = self.game.action_space.n
        self.state_size = self.game.observation_space.shape[0]

        self.state_input = tf.placeholder(tf.float32, [None, self.state_size])
        self.rewards = tf.placeholder(shape=[None], dtype=tf.float32)
        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)

        # Define any additional placeholders needed for training your agent here:

        self.state_value = self.critic()
        self.actor_probs = self.actor()
        self.loss_val = self.loss()
        self.train_op = self.optimizer()
        self.session = tf.Session()
        self.session.run(tf.global_variables_initializer())

    def optimizer(self):
        """
        :return: Optimizer for your loss function
        """
        train = tf.train.AdamOptimizer(0.001).minimize(self.loss_val)
        return train

    def critic(self):
        """
        Calculates the estimated value for every state in self.state_input. The critic should not depend on
        any other tensors besides self.state_input.
        :return: A tensor of shape [num_states] representing the estimated value of each state in the trajectory.
        """
        hiddenSz = 32
        V1 = tf.Variable(tf.random_uniform([self.state_size, hiddenSz], dtype=tf.float32))
        b1 = tf.Variable(tf.random_uniform([hiddenSz], dtype=tf.float32))
        v1Out = tf.nn.relu(tf.matmul(self.state_input, V1) + b1)

        V2 = tf.Variable(tf.random_uniform([hiddenSz, 1], dtype=tf.float32))
        b2 = tf.Variable(tf.random_uniform([1], dtype=tf.float32))
        v2Out = tf.matmul(v1Out, V2) + b2
        return v2Out

    def actor(self):
        """
        Calculates the action probabilities for every state in self.state_input. The actor should not depend on
        any other tensors besides self.state_input.
        :return: A tensor of shape [num_states, num_actions] representing the probability distribution
            over actions that is generated by your actor.
        """
        hiddenSz = 32
        V1 = tf.Variable(tf.random_uniform([self.state_size, hiddenSz], dtype=tf.float32))
        b1 = tf.Variable(tf.random_uniform([hiddenSz], dtype=tf.float32))
        v1Out = tf.nn.relu(tf.matmul(self.state_input, V1) + b1)

        V2 = tf.Variable(tf.random_uniform([hiddenSz, self.num_actions], dtype=tf.float32))
        b2 = tf.Variable(tf.random_uniform([self.num_actions], dtype=tf.float32))
        v2Out = tf.nn.softmax(tf.matmul(v1Out, V2) + b2)
        return v2Out

    def loss(self):
        """
        :return: A scalar tensor representing the combined actor and critic loss.
        """
        indices = tf.range(0, tf.shape(self.actor_probs)[0]) * 2 + self.actions
        actprobs = tf.gather(tf.reshape(self.actor_probs, [-1]), indices)
        advantage = self.rewards - self.state_value
        aloss = -tf.reduce_mean(tf.log(actprobs) * advantage)
        closs = tf.reduce_mean(tf.square(self.rewards - self.state_value))
        loss = aloss + closs
        return loss

    def train_episode(self):
        """
        train_episode will be called 1000 times by the autograder to train your agent. In this method,
        run your agent for a single episode, then use that data to train your agent. Feel free to
        add any return values to this method.
        """
        states, actions, rewards = [], [], []
        gamma = 0.99
        state = self.game.reset()
        for i in range(999):
            state = np.reshape(state, [-1, self.state_size])
            actDist = self.session.run(self.actor_probs, feed_dict={self.state_input: state})
            act = np.random.choice(self.num_actions, p=np.squeeze(actDist))
            states.append(state)
            st1, r, dn, _ = self.game.step(act)
            actions.append(act)
            rewards.append(r)
            state = st1
            if dn:
                disRs = rewards[:]
                prev = 0
                for j in range(1, len(rewards) + 1):
                    disRs[-j] += prev * gamma
                    prev = disRs[-j]

                self.session.run(self.train_op, feed_dict={self.state_input: np.squeeze(states),
                                                           self.actions: actions,
                                                           self.rewards: disRs})
                break
        return np.sum(rewards)


def check_actor(model):
    """
    The autograder will use your actor() function to test your agent. This function
    checks that your actor returns a tensor of the right shape for the autograder.
    :return: True if the model's actor returns a tensor of the correct shape.
    """
    dummy_state = np.ones((10, 4))
    actor_probs = model.session.run(model.actor_probs, feed_dict={
        model.state_input: dummy_state
    })
    return actor_probs.shape == (10, 2)


if __name__ == '__main__':
    # Change __main__ to train your agent for 1000 episodes and print the average reward over the last 100 episodes.
    # The code below is similar to what our autograder will be running.

    learner = A2C()
    sum_reward = 0
    for i in range(1000):
        reward = learner.train_episode()
        if i >= 900:
            sum_reward += reward
    print(sum_reward / 100)
    assert(check_actor(learner))
