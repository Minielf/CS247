The biggest problem I encountered at first is I don't know what should I do in the read function. After reading the tips given in the stencil code and really understand the structure of RNN, I finally get the idea behind this function. Also, lots of shape transforms are used in this assignment. 

The basic steps are the same like the previous assignments. In the forward pass, I use an embedding at first, then use a LSTM cell to achieve the RNN, use dynamic_rnn to get the outputs and use a W and b at last to calculate the logits. Optimizer function is the same as before, AdamOptimizer with learning rate of 1e-3. The loss function is a little bit different from before that we use seq2seq here. In the main function, we need to reshape the input to the BATCH_SIZE * WINDOW_SIZE to feed into the model.