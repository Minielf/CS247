The problem I had before is the loss I have for training data makes sense but the final perplexity is around 10 thousands. The reason for that is I created a new word2id for testing data based on "dev.txt" and process X1, X2 and Y based on this word2id. However, if I use the word2id from testing data, the indices are totally different from the ones in the word2id in the training data. Then I try to process X1, X2 and Y for "dev.txt" based on word2id from training data, problem solved.

The general idea I have is to first process the data to create a list of data with one word and its proceeding two words. In forward_pass, I use W, b and E to do the embedding and calculate the logits. The loss function I use is losses.sparse_softmax_cross_entropy. I use embedding size of 100. In general, the perplexity I get is around 260 and the run-time in the department machine is under 20 mins.